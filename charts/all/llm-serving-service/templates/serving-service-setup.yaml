---
apiVersion: dashboard.opendatahub.io/v1
kind: AcceleratorProfile
metadata:
  name: nvidia-gpu
  namespace: redhat-ods-applications
spec:
  displayName: NVIDIA GPU
  enabled: true
  identifier: nvidia.com/gpu
  tolerations:
  - effect: NoSchedule
    key: odh-notebook
    operator: Exists
---
kind: Secret
apiVersion: v1
metadata:
  name: aws-connection-rag-llm-bucket
  namespace: tenant-ns
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/managed: 'true'
  annotations:
    opendatahub.io/connection-type: s3
    openshift.io/display-name: rag-llm-bucket
data:
  AWS_ACCESS_KEY_ID: minio
  AWS_SECRET_ACCESS_KEY: minio123
stringData:
  AWS_DEFAULT_REGION: us-east-1
  AWS_S3_BUCKET: my-storage
  AWS_S3_ENDPOINT: http://minio-service:9000
type: Opaque
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-7b-instruct-v02
  generation: 1
  namespace: tenant-ns
  finalizers:
    - inferenceservice.finalizers
  labels:
    networking.knative.dev/visibility: cluster-local
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '8'
          memory: 10Gi
        requests:
          cpu: '4'
          memory: 8Gi
      runtime: mistral-7b-instruct-v02
      storage:
        key: aws-connection-rag-llm-bucket
        path: models
---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/accelerator-name: ''
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/template-display-name: vLLM ServingRuntime for KServe
    opendatahub.io/template-name: vllm-runtime
    openshift.io/display-name: vllm
  name: mistral-7b-instruct-v02
  generation: 1
  namespace: tenant-ns
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  containers:
    - args:
        - '--port=8080'
        - '--model=/mnt/models'
        - '--served-model-name={{.Name}}'
        - '--distributed-executor-backend=mp'
      command:
        - python
        - '-m'
        - vllm.entrypoints.openai.api_server
      env:
        - name: HF_HOME
          value: /tmp/hf_home
      image: 'quay.io/modh/vllm@sha256:b51fde66f162f1a78e8c027320dddf214732d5345953b1599a84fe0f0168c619'
      name: kserve-container
      ports:
        - containerPort: 8080
          protocol: TCP
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
---
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: mistral-7b-instruct-v02-predictor
  generation: 1
  namespace: tenant-ns
  labels:
    component: predictor
    networking.knative.dev/visibility: cluster-local
    opendatahub.io/dashboard: 'true'
    serving.kserve.io/inferenceservice: mistral-7b-instruct-v02
spec:
  template:
    metadata:
      annotations:
        internal.serving.kserve.io/storage-initializer-sourceuri: '<scheme-placeholder>://models'
        autoscaling.knative.dev/max-scale: '1'
        prometheus.io/port: '8080'
        openshift.io/display-name: Mistral-7B-Instruct-v0.2
        prometheus.io/path: /metrics
        sidecar.istio.io/inject: 'true'
        internal.serving.kserve.io/storage-spec-key: aws-connection-test-bucket
        autoscaling.knative.dev/min-scale: '1'
        sidecar.istio.io/rewriteAppHTTPProbers: 'true'
        autoscaling.knative.dev/class: kpa.autoscaling.knative.dev
        internal.serving.kserve.io/storage-spec: 'true'
      creationTimestamp: null
      labels:
        component: predictor
        opendatahub.io/dashboard: 'true'
        serving.kserve.io/inferenceservice: mistral-7b-instruct-v02
    spec:
      containerConcurrency: 0
      containers:
        - resources:
            limits:
              cpu: '8'
              memory: 10Gi
            requests:
              cpu: '4'
              memory: 8Gi
          readinessProbe:
            successThreshold: 1
            tcpSocket:
              port: 0
          name: kserve-container
          command:
            - python
            - '-m'
            - vllm.entrypoints.openai.api_server
          env:
            - name: HF_HOME
              value: /tmp/hf_home
          ports:
            - containerPort: 8080
              protocol: TCP
          volumeMounts:
            - mountPath: /dev/shm
              name: shm
          image: 'quay.io/modh/vllm@sha256:b51fde66f162f1a78e8c027320dddf214732d5345953b1599a84fe0f0168c619'
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--served-model-name=mistral-7b-instruct-v02'
            - '--distributed-executor-backend=mp'
      enableServiceLinks: false
      timeoutSeconds: 300
      volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 2Gi
          name: shm
  traffic:
    - latestRevision: true
      percent: 100
