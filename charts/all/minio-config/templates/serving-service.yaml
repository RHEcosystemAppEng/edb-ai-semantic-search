apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: mistral-7b-instruct-v02-predictor
  generation: 1
  namespace: tenant-ns
  labels:
    component: predictor
    networking.knative.dev/visibility: cluster-local
    opendatahub.io/dashboard: 'true'
    serving.kserve.io/inferenceservice: mistral-7b-instruct-v02
spec:
  template:
    metadata:
      annotations:
        internal.serving.kserve.io/storage-initializer-sourceuri: '<scheme-placeholder>://models'
        autoscaling.knative.dev/max-scale: '1'
        prometheus.io/port: '8080'
        openshift.io/display-name: Mistral-7B-Instruct-v0.2
        prometheus.io/path: /metrics
        sidecar.istio.io/inject: 'true'
        internal.serving.kserve.io/storage-spec-key: aws-connection-test-bucket
        autoscaling.knative.dev/min-scale: '1'
        sidecar.istio.io/rewriteAppHTTPProbers: 'true'
        autoscaling.knative.dev/class: kpa.autoscaling.knative.dev
        internal.serving.kserve.io/storage-spec: 'true'
      creationTimestamp: null
      labels:
        component: predictor
        opendatahub.io/dashboard: 'true'
        serving.kserve.io/inferenceservice: mistral-7b-instruct-v02
    spec:
      containerConcurrency: 0
      containers:
        - resources:
            limits:
              cpu: '8'
              memory: 10Gi
            requests:
              cpu: '4'
              memory: 8Gi
          readinessProbe:
            successThreshold: 1
            tcpSocket:
              port: 0
          name: kserve-container
          command:
            - python
            - '-m'
            - vllm.entrypoints.openai.api_server
          env:
            - name: HF_HOME
              value: /tmp/hf_home
          ports:
            - containerPort: 8080
              protocol: TCP
          volumeMounts:
            - mountPath: /dev/shm
              name: shm
          image: 'quay.io/modh/vllm@sha256:b51fde66f162f1a78e8c027320dddf214732d5345953b1599a84fe0f0168c619'
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--served-model-name=mistral-7b-instruct-v02'
            - '--distributed-executor-backend=mp'
      enableServiceLinks: false
      timeoutSeconds: 300
      volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 2Gi
          name: shm
  traffic:
    - latestRevision: true
      percent: 100
